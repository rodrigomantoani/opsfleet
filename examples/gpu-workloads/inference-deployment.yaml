apiVersion: apps/v1
kind: Deployment
metadata:
  name: pytorch-inference
spec:
  replicas: 2  # Multiple replicas to demonstrate GPU sharing
  selector:
    matchLabels:
      app: pytorch-inference
  template:
    metadata:
      labels:
        app: pytorch-inference
    spec:
      runtimeClassName: nvidia
      containers:
      - name: pytorch
        image: pytorch/pytorch:2.1.0-cuda11.8-cudnn8-runtime
        command:
        - "python"
        - "-c"
        - |
          import torch
          import time
          print(f"PyTorch version: {torch.__version__}")
          print(f"CUDA available: {torch.cuda.is_available()}")
          if torch.cuda.is_available():
              print(f"CUDA device: {torch.cuda.get_device_name(0)}")
              while True:
                  # Simulate inference workload
                  x = torch.rand(100, 100).cuda()
                  y = torch.rand(100, 100).cuda()
                  z = torch.matmul(x, y)
                  print("Inference batch processed")
                  time.sleep(1)  # Wait between batches
        resources:
          limits:
            nvidia.com/gpu: "0.25"  # Request quarter of a GPU
          requests:
            memory: "2Gi"
            cpu: "1"
      tolerations:
      - key: nvidia.com/gpu
        operator: Exists
        effect: NoSchedule
      nodeSelector:
        gpu.nvidia.com/class: ampere
